\documentclass[11pt,a4paper]{ivoa}
\input tthdefs

\title{Virtual Observatory and High Energy Astrophysics}

% see draft note here:

% see ivoatexDoc for what group names to use here; use \ivoagroup[IG] for
% interest groups.
\ivoagroup{DM}

\author{HE club}

\editor{Mathieu Servillat}

% \previousversion[????URL????]{????Concise Document Label????}
\previousversion{This is the first public release}

\usepackage{longtable}
%\usepackage{booktabs} % For prettier tables
\usepackage{lscape}
%\usepackage{minted}

\setlength {\marginparwidth }{2cm}
\usepackage{todonotes}

\begin{document}

\begin{abstract}
Virtual Observatory and High Energy Astrophysics
\end{abstract}


\section*{Acknowledgments}

We acknowledge support from the ESCAPE project funded by the EU Horizon 2020 research and innovation program (Grant Agreement n.824064).
Additional funding was provided by the INSU (Action Sp\'ecifique Observatoire Virtuel, ASOV), the Action F\'ed\'eratrice
CTA and the Action Pluriannuelle Incitatrice Astrophysioque des processus de Hautes \'Energies at the Observatoire de
Paris, and the Paris Astronomical Data Centre (PADC).

\section*{Conformance-related definitions}

The words ``MUST'', ``SHALL'', ``SHOULD'', ``MAY'', ``RECOMMENDED'', and
``OPTIONAL'' (in upper or lower case) used in this document are to be
interpreted as described in IETF standard RFC2119 \citep{std:RFC2119}.

The \emph{Virtual Observatory (VO)} is a
general term for a collection of federated resources that can be used
to conduct astronomical research, education, and outreach.
The \href{https://www.ivoa.net}{International
Virtual Observatory Alliance (IVOA)} is a global
collaboration of separately funded projects to develop standards and
infrastructure that enable VO applications.


\section{Introduction}

% We should introduce the purpose of the note in distribution and access of event list data products. Science cases should be focused to highlight that.

High Energy (HE) astronomy typically includes X-ray astronomy, gamma-ray astronomy, neutrino astronomy, and studies of cosmic rays. This domain is now sufficiently developed to provide high level data such as catalogs, images, including full-sky surveys for some missions, and sources properties in the shape of spectra and time series.
Such high level, HE observations have been included in the VO, via data access endpoints provided by observatories or by agencies and indexed in the VO Registry.
%Some high-energy (HE) data is already available via the VO. Images, time series, and spectra may be described with Obscore and access.

However, after browsing this data, users may want to download lower level data and reapply data reduction steps relevant to their Science objectives. A common scenario is to download HE "event" lists, i.e. lists of detected events on a HE detector, that are expected to be detection of particles (e.g. a HE photon), and the corresponding calibration files, including Instrument Response Functions (IRFs). The findability and accessibility of these data via the VO is the focus of this note.

We first report typical use cases for data access and analysis of data from current HE observatories. From those use cases, we note that some existing IVOA Recommendations are of interest to the domain. They should be further explored by HE observatories. We then discuss how standards could evolve to better integrate specific aspects of HE data, and if new standards should be developed.

\subsection{Objectives of the document}

The main objective of the document is to analyse how HE data can be better integrated to the VO.

We  first identify and expose the specificities of HE data from several HE observatories. Then we intend to illustrate how HE data is or can be handled using current IVOA standards. Finally, we explore several topics that could lead to HE specific recommandations.

A related objective is to provide a context and a list of topics to be further discussed within the IVOA by a dedicated HE Interest Group.


\subsection{Scope of the document}

This document mainly focuses on HE data discovery through the VO, with the identification of common use cases in the HE Astrophysics domain, which provides an insight of the specific metadata to be expose through the VO for HE data.

To some extend, all current existing IVOA recommandation could be discussed in this document in the HE context.



% \subsection{Role within the VO Architecture}

% \begin{figure}
% \centering

% % As of ivoatex 1.2, the architecture diagram is generated by ivoatex in
% % SVG; copy ivoatex/archdiag-full.xml to role_diagram.xml and throw out
% % all lines not relevant to your standard.
% % Notes don't generally need this.  If you don't copy role_diagram.xml,
% % you must remove role_diagram.pdf from SOURCES in the Makefile.

% \includegraphics[width=0.9\textwidth]{role_diagram.pdf}
% \caption{Architecture diagram for this document}
% \label{fig:archdiag}
% \end{figure}

% Fig.~\ref{fig:archdiag} shows the role this document plays within the
% IVOA architecture \citep{2010ivoa.rept.1123A}.


\section{High Energy observatories and experiments}
%XMM use case scenario
%Données attachées ? data link?

There are various observatories, either from ground- or space-based, that distribute high-energy data with different level of involvement in the VO. We list here the observatories currently represented in the VO HE group. There are also other observatories that are connected to the VO in some way, and may join the group discussions at IVOA.


\subsection{Gamma-ray programs}

\subsubsection{CTAO}

The Cherenkov Telescope Array Observatory (CTAO) is the next generation ground-based instrument for gamma-ray astronomy at Very-High Energies (VHE). With tens of telescopes located in the northern and southern hemispheres, the CTAO will be the first open ground-based gamma-ray observatory and the world’s largest and most sensitive instrument to study high-energy phenomena in the Universe.
Building on the technology of current generation ground-based gamma-ray detectors (H.E.S.S., MAGIC and VERITAS), the CTAO will be between five and 10 times more sensitive and have unprecedented accuracy in its detection of high-energy gamma rays.

CTAO will distribute data as an open observatory, for the first time in this domain, with calls for proposals and publicly released data after a proprietary period. CTAO will ensure that the data provided will be FAIR: Findable, Accessible, Interoperable and Reusable, by following the FAIR Principles for data management \citep{Wilkinson2016}. In particular, because of the complex data processing and reconstruction step, the provision of provenance metadata for CTAO data has been a driver for the development of a provenance standard in Astronomy.

CTAO will also ensure VO compatibility of the distributed data and access systems. CTAO participated to the ESCAPE European Project, and is now part of the ESCAPE Open Collaboration to face common challenges for Research Infrastructure in the context of cloud computing, including data analysis and distribution.

A focus of CTAO is to distribute in this context their Data Level 3 (DL3) datasets, that correspond to lists of Cherenkov events detected by the telescopes along with the proper IRFs. CTAO is planning an internal and a public Science Data Challenge, which represent opportunities to build "VO inside" solutions.

\subsubsection{H.E.S.S}
\label{sec:hess}

H.E.S.S. is a system of Imaging Atmospheric Cherenkov Telescopes located in Namibia that investigates cosmic gamma rays in the energy range from 10s of GeV to 100s of TeV. It is constituted of four telescopes officially inaugurated in 2004, and a much larger fifth telescope operational since 2012, extending the energy coverage towards lower energies and further improving sensitivity.

The H.E.S.S. collaboration operates the telescopes as a private experiment and published mainly high level data, i.e. images, time series and spectra in scientific publications after dedicated analyses.

In September 2018, the H.E.S.S. Collaboration has, for the first time and unique time, released a small subset of its archival data in Flexible Image Transport System (FITS) format, an open file format widely used in astronomy. The release consists of Cherenkov event-lists and IRFs for observations of various well-known gamma-ray sources
\citep{hess-zenodo.1421098}.

This test data collection has been registered in the VO via a TAP service hosted at the Observatoire de Paris, with a tentative ObsCore description of each dataset. We hope that in the future, the H.E.S.S. legacy archive will be published in a similar way and accessible through the VO.



\subsection{X-ray programs}

\subsubsection{Chandra}\label{sec:chandra}

Part of NASA's fleet of ``Great Observatories'', the Chandra X-ray Observatory (CXO) was launched in 1999 to observe the soft X-ray universe in the 0.1 to 10 keV energy band.  Chandra is a guest observer, pointed-observation mission and obtains roughly 800 observations per year using the Advanced CCD Imaging Spectrometer (ACIS) and High Resolution Camera (HRC) instruments.  Chandra provides high angular resolution with a sub-arcsecond on-axis point spread function (PSF), a field of view up to several hundred square arcminutes, and a low instrumental background. The Chandra PSF varies with X-ray energy and significantly with off-axis angle, increasing to R50 $\sim$25 arcsec at the edge of the field of view. A pair of transmission gratings can be inserted into the X-ray beam to provide dispersed spectra with E/DeltaE $\sim$1000 for bright sources.
The Chandra spacecraft normally dithers in a Lissajous pattern on the sky while taking data, and this motion must be removed from the time-resolved X-ray event lists when constructing X-ray images using the motion of optical guide stars tracked by the Aspect camera.

The Chandra X-ray Center (CXC) processes the spacecraft data through a set of Standard Data Processing Level 0 through Level 2 pipelines.  These pipelines perform numerous steps including decommutating the telemetry data, applying instrument calibrations (e.g., detector geometric, time- dependent gain, and CCD charge transfer efficiency [CTI] corrections, bad and hot pixel flagging), computing and applying the time-resolved Aspect solution to de-dither the motion of the telescope, identifying good time intervals (GTIs), and finally filtering out bad times and X-ray events with bad status.  All data products are archived in the Chandra Data Archive (CDA) in FITS format following HEASARC OGIP standards;  see also \S~\ref{sec:ogip}.  The CDA manages the proprietary data period (currently 6 months, after which the data become public) and provides dedicated interactive and IVOA-compliant interfaces to locate and download datasets.

The CXC also provides the Chandra Source Catalog, which in the latest release (2.1) includes data for $\sim$407K unique X-ray sources on the sky and more than 2.1 million individual detections and photometric upper limits.  For each X-ray source and detection, the catalog provides a detailed set of more than 100 tabulated positional, spatial, photometric, spectral, and temporal properties.  An extensive selection of individual observation, stacked-observation, detection region, and master source FITS data products (e.g., RMFs, ARFs, PSFs, spectra, light curves, aperture photometry MPDFs) are also provided that are directly usable for further detailed scientific analysis.

Finally, the CXC distributes the CIAO data analysis package to allow users to recalibrate and analyze their data.  A key aspect of CIAO is to provide users the ability to create instrument responses (RMFs, ARFs, PSFs, instrument and exposure maps, etc.) for their observations using their choice of spectral models and weightings.  The Sherpa modeling and fitting package supports N-dimensional model fitting and optimization in Python, and supports advanced Bayesian Markov chain Monte Carlo analyses.  


\subsubsection{XMM-Newton}

The European Space Agency's (ESA) X-ray Multi-Mirror Mission (XMM-Newton) was launched in 1999. XMM-Newton is ESA's second cornerstone of the Horizon 2000 Science Program. It carries 3 high throughput X-ray telescopes with an unprecedented effective area, and an optical monitor, dedicated to the study of celestial X-ray sources.

\todo[inline]{To be completed: XMM catalogs, data... and VO access.}


\subsubsection{SVOM}

The SVOM mission (Space-based multi-band astronomical Variable Objects Monitor) is a Franco-Chinese mission dedicated to the study of the most distant explosions of stars, the gamma-ray bursts. It is to be launched in 2024.

\todo[inline]{To be completed}



\subsection{KM3Net and neutrino detection}

The KM3NeT neutrino detectors are an array of water-based Cherenkov detectors currently under construction in the deep Mediterranean Sea. With its two sites off the French and Italian coasts the KM3NeT collaboration aims at single particle neutrino detection for neutrino physics with the more densely instrumented ORCA detector in the GeV to TeV range, and high-energy astrophysics with the ARCA detector in the TeV range and above.

Using Earth as a shield from atmospheric particle interference by searching for upgoing particle tracks in the detectors, the measurement of astrophysical neutrinos can be performed almost continuously for a wide field of view that covers the full visible sky. For these particle events, extensive Monte Carlo simulations are performed to evaluate the statistical significance towards the various theoretical assumptions for galactic or cosmic neutrino signals.

During the construction phase, the KM3NeT collaboration develops its interfaces for open science and builds on the data gathered by its predecessor ANTARES, from which neutrino event lists have already been published on the KM3NeT VO server as TAP service. However, reproducibility of the searches for point-like sources require information derived from simulations like background estimate, point spread function and detector acceptance which require linking to the actual event list and interpretation for a given observation, usually as neutrino flux limits for non-significant detection attributable to background rather than an observation.

With multiple detectors targeting high-energy neutrinos like IceCube, ANTARES, KM3NeT, Baikal and future projects, the chance to detect a significant amount of cosmic and galactic neutrinos increases, requiring an integrated approach to link event lists with instrument responses and to correctly interpret observation time and flux expectations.



%\subsection{Gravitational wave detection}



% mireille : what is specific for the community in terms of data interpretation and computation steps

\section{Common practices in the High Energy community}

\subsection{Data flow specificities}

\subsubsection{Event-counting}

Observations of the Universe at high energies are based on techniques that are radically different compared to the optical, or radio domain. HE observatories are generally designed to detect particles, e.g. individual photons, cosmic-rays, or neutrinos, with the ability to estimate several characteristics of those particles. This technique is generally named \textbf{event counting}, where an event has some probability of being due to the interaction of an astronomical particle with the detectors.

The data corresponding to an \textbf{event} is first an instrumental signal, which is then calibrated and processed to estimate event characteristics such as a time of arrival, coordinates on the sky, and the energy proxy associated to the event. Several other intermediate and qualifying characteristics can be associated to a detected event.

When observing during an interval of time, the data collected is a list of the detected events, named an \textbf{event list} in the HE domain, and event-list in this document.

%HE projects already have data formats in use to transport the results of observations together with the necessary instrument response files.

%Such response files depend on the way raw event lists are combined together; they are essential for the calibration steps that will help to produce calibrated event-lists in position, time and energy.


\subsubsection{Data levels}

After detection of events, data processing steps are applied to generate data products. We typically distinguish at
least three main data levels.

\begin{itemize}
    \item[1] An event-list with calibrated temporal and spatial characteristics, e.g. sky coordinates for a given epoch,
    event arrival time with time reference, and a proxy for particle energy.
    \item[2] Binned and/or filtered event list suitable for preparation of science images, spectra or light-curves. For
    some instruments, corresponding instrument responses associated with the event-list, calculated but not yet applied
    (e.g, exposure maps, sensitivity maps, spectral responses).
    \item[3] Calibrated/astrophysical maps, spectral energy distributions for a source, or light-curves in physical units.
    \item[4] An additional data level may correspond to catalogs, e.g. a source catalog pointing to several data products
    (e.g. collection of L3/DL5 products) with each one corresponding to a source, catalog of source models generated with
    a uniform analyse.
\end{itemize}

However, the definitions of these data levels can vary significantly from facility to facility, and may not map directly
to separate ObsCore calib\_levels.

For example, in the VHE Cherenkov astronomy domain (e.g. CTA), the data levels listed above are labeled DL3\footnote{lower
level data (DL0--DL2), that are specific to the used instrumentation (IACT, WCD), are reconstructed and filtered, which
constitute the events lists called DL3.} to DL5.  However, for Chandra X-ray data, the first two levels correspond to L1
and L2 data products (excluding the responses), while transmission-grating data products are designated L1.5 and source
catalog and associated data products are all designated L3.

\subsubsection{Background signal}

Observations in HE/VHE may contain a high background component, that may be due to instrument noises, cosmic rays,
unresolved astrophysical sources, emission from extended regions or other terrestrial sources producing particles similar
to the signal. The characterization and estimation of this background may be particularly important to apply corrections
during the analysis of a source signal.

In the VHE domain with the IACT, WCD and neutrino techniques, the main source of background at the DL3 level is from
cosmic-ray induced events. The case of unresolved astrophysical sources, emission from extended regions are treated as
 models of gamma-ray or neutrino emission. In the X-ray domain, contributions to background can include an instrumental
component, the local radiation environment (i.e. space weather) which can change dynamically, and may include the
cosmological background due to unresolved astrophysical sources, depending on the spatial resolution of the instrument.

\subsubsection{Time intervals}

Depending on the stability of the instruments and observing conditions, a HE observation can be decomposed into several
intervals of time that will be further analysed. For example, Stable Time Intervals (STI) are defined in Cherenkov
astronomy to characterize periods of time during which the IRFs are stable and under control. In the X-ray domain, Good
Time Intervals (GTI) are computed to exclude time periods where data are missing or invalid, and may be used to reject
periods impacted by high radiation, e.g. due to space weather. In contrast, for neutrino detectors or WCDs, relevant
observation periods can cover up to several years due to the instrument stability.


\subsubsection{Instrument Response Functions}

Though an event-list can contain calibrated physical values, typically the data still has to be corrected for the
photometric, spectral, spatial, and/or temporal responses of the instruments used to yield scientifically interpretable
information. The IRFs provide mappings between the physical properties of the source and the observables, and so enable
estimation of the former (such as the real flux of particles arriving at the instrument, the spectral distribution of
the particle flux, and the temporal variability and morphology of the source).  Note that the small number of particles
detected in many types of HE observations (i.e., Poisson regime) implies that the IRFs may not be directly invertible,
so that techniques such as forward-folding fitting are needed to estimate the physical properties of the source from the
observables. Depending on the instrument, this may imply that some IRFs cannot be easily pre-computed because they may
depend on details (e.g. the width of the spatial integration region) of the scientific analysis to be performed.

\subsubsection{Granularity of data products}

In order to allow for multi-wavelength data discovery of HE data products and compare observations across different
regimes, it seems appropriate to distribute the metadata in the VO ecosystem together with an access link to the data
file in community format for finer analysis.

Where feasible, the efficient granularity for distributing HE data products seems to be the full combination of data
and associated IRFs. Depending on the instrument, some IRFs may need to be (re-)computed by a service or tool after
parameter selection by the user, so inclusion of additional files that are required for these steps should be included
in the package where appropriate.

% mir already mentionned above why we should consider IRF
%The coverage information, i.e. how the data product spans on the sky coordinates, and along time and energy axis, is
%an important criterium for data selection. In the case of HE observations, these parameters vary depending on the
%selected good time intervals.
% to be developed

The event-list dataset is generally stored as a table, with one row per candidate detection (event) and several columns
for the observed and/or estimated physical parameters (e.g. arrival time, position (on detector or in the sky), energy
or pulse height, and additional properties such as errors or flags that are project-dependent).

The list of columns present in the event-list is for example described in the data format in use in the HE domain,
such as OGIP or GADF as introduced below. The data formats in use generally describe the event-list data together with
the IRFs and other relevant information, such as: Stable or Good Time Interval, dead time, ...

%From an observation file, the event data can lead to several data products results, depending on the parameter
%selections induced from the knowledge of the IRFa.


\subsection{Work flow specificities}

\subsubsection{Event selection}

When processing an event-list, it is common to perform an optimal selection of the events that are more likely to be
due to the incident particles expected {\bf this is not the case for IACT/WCD. To be changed}. This selection may depend
on the source targeted or on the science objectives. {\bf The following is true in general} The selection can be
performed on the event characteristics, e.g. time, energy or more specific indicators (patterns, shape...)


\subsubsection{Assumptions and probabilistic approach}

In order to produce advanced data products like light curves or spectra with physical units, assumptions about the kind
of particles, background and noises, spatial/spectral/temporal characteristics of the sources must be introduced. This
is one of the main drivers for enabling a full and well described access to event-list data, as scientific analyses
generally start at this data level. Then, users process these L1/DL3 data with open science analysis tools to derive
astrophysical products associated to the users' analysis use case.

\subsection{Data formats}
\label{sec:data_formats}

\subsubsection{{OGIP}}\label{sec:ogip}

NASA's HEASARC FITS Working Group was part of the Office of Guest Investigator Programs, or OGIP, and created in the
1990's the multi-mission standards for the format of FITS data files in NASA high-energy astrophysics. Those so-called
OGIP recommendations\footnote{\url{https://heasarc.gsfc.nasa.gov/docs/heasarc/ofwg/ofwg_recomm.html}} include standards
on keyword usage in metadata, on the storage of spatial, temporal, and spectral (energy) information, and representation
of response functions, etc. These standards predate the IVOA but include some VO concepts as data models, vocabularies,
provenance, as well as the corresponding FITS serialization specification.

The purpose of these standards was to allow all mission data archived by the HEASARC to be stored in the same data format
and be readable by the same software tools. \S~\ref{sec:chandra} above, for example, describes the Chandra mission
products, but many other smaller projects do so as well. Because of the OGIP standards, the same software tools can be
used on all of the high-energy mission data that follow them. There are now some thirty plus different mission datasets
archived by NASA following these standards and different software tools that can analyze any of them.

Now that the IVOA is defining data models for spectra and time series, we should be careful to include the existing OGIP
standards as special cases of what are developed to be more general standards for all of astronomy.

\subsubsection{GADF and VODF}
\label{sec:GADF}

The used data formats for gamma-ray astronomy\footnote{\url{https://gamma-astro-data-formats.readthedocs.io/}} (GADF) is a community-driven initiative created in the
2010s that has defined a common and open high-level data format for gamma-ray instruments \citep{2017AIPC.1792g0006D,2021-DF}.
GADF is based on the OGIP standards and is specialised for VHE data. It is designed for a serialization into FITS files.
This format became a standard for the whole VHE gamma-ray community (IACT, WCD) and it has been demonstrated that it can
be used also for neutrino instruments. With the establishment of the GADF format, shared science analysis tools can be
used by all, which enable for the first time full and coherent joint analyses of data from different facilities.

The Very-high-energy Open Data Format\footnote{\url{https://vodf.readthedocs.io/}} (VODF) is an open data model and
format for VHE gamma-ray and neutrino astronomy. Its goal is to provide a standard set of file formats and standards
for data starting at the reconstructed event level as well as higher-level products such as N-dimensional binned data
cubes (including sky images, light curves, and spectra) and source catalogues. It will allow a full respect of the FAIR
principles whatever the data levels and aims to follow as much as possible the IVOA standards. In particular, provenance
recommendations will be introduced.

It is however anticipated that the formatting of the source model description is a bottleneck for the VHE field, and
as well for the HE field. Indeed, each facility, catalog providers or science analysis tool is currently its own format
to describe the source models. Also, it appears that some science analysis tools even do not serialise such information,
rendering them not compliant to the FAIR4RS principles. The data formatting task impacts then also the teams of the main
analysis libraries.
%\todo[inline]{To be completed:  Bruno}

\subsection{Tools for data extraction and visualisation}
\label{sec:tools}

HE data is particularly complex and diverse between the L1 X-ray data and DL3 gamma-ray/neutrino data, and also between
facilities. Specific open tools to process the data are in use by the community: CIAO for Chandra, SAS fro XMM-Newton,
Gammapy for gamma-ray data. Private tools owned by experiments also exist, but they will not be mentioned here.

Their development history shows different requirements and objectives. Their compliance to the FAIR4RS principles is not
yet perfect, even if the written data are more and more FAIR. The used and written metadata do not follow exactly the same
conventions or format, in particular for the provenance or the history. The format of the source models is also not fully
compatible.

Nowadays, those facilities libraries and multi-purpose packages (e.g. XSpec, Sherpa, 3ML, Gammapy) can generally handle
data from several other observatories. This is handled either by having data formats with some level of commonalities or
by handling different data format from facilities. The multi-instrument data analyses are becoming crucial for some
science use cases and the existence of this feature is  greatly appreciated by the community. It opens naturally to
multi-wavelength analyses with an uniform statistical framework and identical systematic errors from the analysis tools.

This feature leads to the library teams to improve their computing performance, their FAIR4RS compliance, in addition to
the use of new software tools coming from the explosion of open Python projects (e.g. astropy, astroquery, jax, numpyro,
ArviZ). New algorithms on minimization, high performance computing (GPUs, parallelism on clusters), statistical analyses,
time series, data access, serialization, data visualisation are developed in the context of the Open Science explosion
and are enriching the features of the HE software. The interoperability between libraries is becoming an important wished
feature, on top of the one of the multi-instrument handling.

In this context, one should not ignore the need to render accessible data from astrophysical simulations in order to test
on real data different astrophysical models. Data format for the simulations outputs should be also considered\footnote{The
XSPEC library contains many models resulting from such theoritical models, even if the lack of traceability or
reproducibility for some starts to be problematic.} in order to be handled correctly by the science analysis tools.

%\todo[inline]{To be completed (e.g. ???)}

% mireille : to be discussed
%??? naïve question : what would be the benefit to convert science ready event table data to VOTable?
%BKH: VOTable are in XML, which is not very great! We generally prefer YAML format. Otherwise, GADF defines a format
% directly with the FITS standards, without passing via this intermediate format. The use of a .ftts and not a .xml
% permits also an easy astrophysical visualization (maps) with tools like DS9, FV, etc... But it is true that the
% handling of metadata by FITS is more and more limited (tricks should be used)
%Would TOPcat, Aladin, etc. allow more preview steps  , xmatch, multi-wavelength analysis ?


\section{Use Cases}

\subsection{UC1: re-analyse event-list data for a source in a catalog}

After the selection of a source of interest, or a group of sources, one may access different HE data products such as images, spectra and light-curves, and then want to download the corresponding event-lists and calibrations to further analyse the data.

%\todo[inline]{To be completed (e.g. Paula, Laurent)}
One of the characteristics of the HE data is that, contrary to what is usually done in optics for example, their optimal use requires providing users with a view of the processing that generated the data. This implies providing ancillary data, products with different calibration levels, and possibly linking together products issued by the same processing.
%(LM)


\subsection{UC2: observation preparation}

When planning for a new HE observation, one needs to search for any existing event-list data already available in the targeted sky regions, and assess if this data is sufficient to fulfill the science objectives.

\todo[inline]{To be completed (e.g. Bruno)}


\subsection{UC3: transient or variable sources}


\todo[inline]{To be completed (e.g. Ada)}


\subsection{UC4: Multi-wavelength and multi-messenger science}

Though there are scientific results based on HE data only, the multi-wavelength and multi-messenger approach is particularly developed in the HE domain. An astrophysical source of HE radiations is indeed generally radiating energy in several domains across the electromagnetic spectrum and may be a strong source of other particles. It is not rare to observe a HE source in radio and to look for counterparts in the infrared, optical or UV domain. Spectroscopy is also widely used to identify HE sources.

The HE domain is thus confronted to different kinds of data types and data archives, which leads to interesting use cases for the development of the VO.

\todo[inline]{To be completed (e.g. Bruno)}


\subsection{Examples of multi-wavelength analysis}

\subsubsection{Multiple Imaging Atmospheric Cherenkov Telescopes extraction example}

In order to exploit high energy data across a large interval of energy values, and from various IACTs, there is a need to harmonise metadata description.
Datasets can then be mixed together to create a fused event-list dataset, to expand the analysis along the spectral energy axis and study the spectral behaviour of an astronomical object.

This was proposed in \citep{2019A&A...625A..10N} by a group of HE astronomers of various HE facilities.
%This work used event-list data products as an input from different facilities (MAGIC, H.E.S.S., FACT, VERITAS, etc...).  data for the Crab Nebula computed from the Maximum likelihood functions of each event depending on the IRFs properties.
In this work, the authors implemented a prototypical data format (GADF) for a small set of MAGIC, VERITAS, FACT, and H.E.S.S. Crab nebula observations, and they analyzed them with the open-source Gammapy software package. By combining data from Fermi-LAT, and from four of the currently operating imaging atmospheric Cherenkov telescopes, they produced a joint maximum likelihood fit of the Crab nebula spectrum.

Such a work has been more recently extended with the HAWC data \citep{2022A&A...667A..36A}, and included neutrino data in a common CTA and KM3NeT source search \citep{unbehaun2023prospects}.


\section{IVOA standards of interest for HE}

\subsection{IVOA Recommendations}

\subsubsection{ObsCore and TAP}

Event-list datasets can be described in ObsCore using a dataproduct\_type set to "event". However, this is not widely used in current services, and we observe only a few services with event-list datasets declared in the VO Registry, and mainly the H.E.S.S. public data release (see \ref{sec:hess}).

As services based on the Table Access Protocol \citep{2019ivoa.spec.0927D} and ObsCore are well developed within the VO, it would be a straightforward option to discover HE event-list datasets, as well as multi-wavelength and multi-messenger associated data.

Here is the evaluation of the ObsCore metadata for distributing high energy data set, some features being re-usable as such, and some other features requested for addition or re-interpretation.


\subsubsection{DataLink}

%\todo[inline]{To be completed (e.g. François)} proposed below by FB (2024-01-31)

DataLink specification \citep{2023ivoa.spec.1215B} defines a \{links\} endpoint providing the possibility to link several
access items to each row of the main response table. These links are described and stored in a second
table. In the case of an ObsCore response each dataset can be linked this way (via the via the access\_url
FIELD content) to previews, documentation pages, calibration data as well as to the dataset itself.
Some dynamical links to web services may also be provided. In that case the service input parameters are
described with the help of a "service descriptor" feature as described in the same DataLink specification.

\subsubsection{HiPS}

Several HE observatories are well suited for sky survey, and the Hierarchical Progressive Survey (HiPS) standard is well suited for sky survey exploration. We note that the Fermi facility provides a useful sky survey in the GeV domain.


\subsubsection{MOCs}

Cross-correlation of data with other observations is an important use case in the HE domain. Using the Multi-Order Coverage map (MOC) standard, such operations become more efficient. Distribution of MOCs associated to HE data should thus be encouraged and especially ST-MOCs (space + time coverage)
that make easier the study of transient phenomena.
% (LM)

\subsubsection{MIVOT}

Model Instances in VOTables (MIVOT) defines a syntax to map VOTable data to any model serialized in VO-DML. The annotation operates as a bridge between the data and the model. It associates the column/param metadata from the VOTable to the data model elements (class, attributes, types, etc.) [...]. The data model elements are grouped in an independent annotation block complying with the MIVOT XML syntax. This annotation block is added as an extra resource element at the top of the VOTable result resource. The MIVOT syntax allows to describe a data structure as a hierarchy of classes. It is also able to represent relations and composition between them. It can also build up data model objects by aggregating instances from different tables of the VOTable.
In the case of HE data, this annotation pattern, used together with the MANGO model, will help to make machine-readable quantities that are currently not considered in the VO, such as the hardness ratio, the energy bands, the flags associated with measurements or  extended sources.
% (LM)

\todo[inline]{To be completed}



\subsubsection{Provenance}

\todo[inline]{To be completed (e.g. Mathieu)}


\subsection{Data Models in working drafts}

The HE domain and practices could serve as use cases for the developments of data models, such as Dataset DM, Cube DM or MANGO DM.


\section{Topics for discussions in an Interest Group}


\subsection{Definition of a HE event in the VO}
\label{sec:event-bundlle-or-list}

\subsubsection{Current definition in the VO}

The IVOA standards incude the concept of event-list, for example in ObsCore v1.1 \citep{2017ivoa.spec.0509L}, where event is a dataproduct\_type with the following definition:
\begin{quote}
    \textbf{event}: an event-counting (e.g. X-ray or other high energy) dataset of some sort. Typically this is instrumental data, i.e., "event data". An event dataset is often a complex object containing multiple files or other substructures. An event dataset may contain data with spatial, spectral, and time information for each measured event, although the spectral resolution (energy) is sometimes limited. Event data may be used to produce higher level data products such as images or spectra.
\end{quote}

More recently, a new definition was proposed in the product-type vocabulary\footnote{\url{https://www.ivoa.net/rdf/product-type}} (draft):
\begin{quote}
    \textbf{event-list}: a collection of observed events, such as incoming high-energy particles. A row in an event list is typically characterised by a spatial position, a time and an energy.
\end{quote}

Such a definition remains vague and general, and could be more specific, including a definition for a HE event, and the event-list data type.

\subsubsection{Proposed definition to be discussed}

A first point to be discuss would be to converge on a proper definition of HE specific data products:
\begin{itemize}
    \item Propose definitions for a product-type \textbf{event-list}: A collection of observed events, such as incoming high-energy particles, where an event is generally characterised by a spatial position, a time and a spectral value (e.g. an energy, a channel, a pulse height).
    \item Propose definitions for a product-type \textbf{event-bundle}: An event-bundle dataset is a complex object containing an event-list and multiple files or other substructures that are products necessary to analyze the event-list. Data in an event-bundle may thus be used to produce higher level data products such as images or spectra.
\end{itemize}

An ObsCore erratum could then propose to change event for event-list and event-bundle.

The precise content of an event-bundle remains to be better defined, and may vary significantly from a facility to another.

For example, Chandra primary products distributed via the Chandra Data Archive include around half a dozen different types of products necessary to analyze Chandra data (for example, L2 event-list, PHA spectrum, Aspect solution, bad pixel map, spacecraft ephemeris, V\&V Report). It is also possible to retrieve secondary products, containing more products that are needed to recalibrate the data with updated calibrations.


\subsection{ObsCore metadata description of an event-list}
\label{sec:obscore}

%%%% texte by Mireille to be checked and merged : start %%
%\include{ObscoreReviewforVOHEcontext_Mireille Louys}
%I have some items to add in the various categories well defined by Mathieu
%%%%%%%%%%texte by Mireille to be merged : end %%

%\subsubsection{Mandatory fields}

\subsubsection{Usage of the mandatory terms in ObsCore}

ObsCore \citep{2017ivoa.spec.0509L} can provide a metadata profile for a data product of type event-list and a qualified access to the distributed file using the Access class from ObsCore (URL, format, file size).

In the ObsCore representation, the event-list data product is described in terms of curation, coverage and access. However, several properties are simply set to NULL following the recommendation: Resolutions, Polarization States, Observable Axis Description, Axes lengths (set to -1)...

We also note that some properties are energy dependent, such as the Spatial Coverage, Spatial Extent, PSF.

\todo[inline]{TODO: show a table with all reused terms , and provide an example}

\begin{itemize}
    \item dataproduct\_subtype = DL3, maybe specific data format (VODF)
    \item calib\_level = between 1 and 2
    \item obs\_collection could contain many details : obs\_type (calib, science), obs\_mode (subarray
configuration), pointing\_mode, tracking\_type, event\_type, event\_cuts, analysis\_type…
    \item s\_ra, s\_dec = maybe telescope pointing coordinates
    \item target\_name : several targets may be in the field of view
    \item s\_fov, s\_region, s\_resolution, em\_resolution... all those values are energy dependent, one should specifiy that the value is at a given energy, or within a range of values.
    \item em\_min, em\_max : add fields expressed in energy (e.g. eV, keV or TeV)
    \item t\_exptime : ontime, livetime, stable time intervals... maybe a T-MOC would help
    \item facility\_name, instrument\_name : minimalist, would be e.g. CTAO and a subarray.
\end{itemize}


\subsubsection{Metadata re-interpretation for the VOHE context}

\paragraph{observation\_id}
In the current definition of ObsCore, the data product collects data from one or several observations. The same happens in HE context.

\paragraph{access\_ref, access\_format}
The initial role of this metadata was to hold the access\_url allowing data access.
Depending on the packaging of the event bundle in one compact format (OGIP, GADF, tar ball, ...)
or as different files available independently in various urls, a datalink pointer can be used for accessing the various parts of IRFs, background maps, etc.
Then in such a case the value for access\_format should be "application/x-votable+xml;content=datalink". The format itself of the data file is then given by the datalink parameter "content-type".
See next section \ref{sec:datalink}.

\paragraph{o\_ucd}
For the even-list table, we can consider all measures stored in columns values have been observed .
The nature of items along time, position and energy axis are identifed in Obscore with ucd as 'time', 'pos.eq.*', 'em.*'
and counted as t\_xel, s\_xel1, s\_xel2, em\_xel which correspond to the number of rows/events candidates observed.

The signal observed is the result of event counting and would be PHA (Pulse height amplitude at detector level) or a number of counts for photons or particles, or a flux, etc.., depending on the data calibration level considered.
ObsCore uses o\_ucd to characterise the nature of the measure.
various UCDs are used for that: o\_ucd=phys.count, phot.count, phot.flux, etc. there is currently no UCD defined for a raw measure like PulseHeightAmplitude, but if needed this can be requested for addition in the UCDList vocabulary. See VEP-UCD-15\_pulseheight.txt proposed at \url{'https://voparis-gitlab.obspm.fr/vespa/ivoa-standards/semantics/vep-ucd/-/blob/master/'}.

Note that these parameters vary between the dataset of calib\_level of 1 (Raw) to the a more advanced data products (calib\_level 2 or 3), which are filtered and rebinned from the original raw event-list.


\subsubsection{Metadata addition required}

\paragraph{ev\_number}
The event list contains a number of rows, representing detections candidates, that have no metadata keyword yet in Obscore.
We propose 'ev\_number' to record this.
In fact the t\_xel, s\_xel1 and s\_xel2, em\_xel elements do not apply for an event list in raw count as it has not been binned yet.

\paragraph{Adding MIME-type to access\_format table}
As seen in section \ref{sec:data_formats} current HE experiments and observatories use their community defined data format for data dissemination.
They encapsulate the event-list table together with ancillary data dedicated to calibration and observing configurations and parameters.
Even if the encapsulation is not standardized between the various projects, it is useful for a client application to rely on the access\_format property in order to send it to an appropriate visualizing tool.

Therefore these can be included in the MIME-type table of ObsCore section 4.7. suggestion for new terms like :
\begin{itemize}
\item application/x-fits-ogip ...
\item application/x-gadf  ...
\item application/x-vodf  ...
\end{itemize}
\todo[inline]{to be completed with proper definition}

\paragraph{energy\_min, energy\_max}
It is not user-friendly for the user to select dataset according to an energy range when the spectral axis is expressed in wavelength and meters. The units and quantities are not familiar to this community.
Moreover the numerical representation of the spectral range in em\_min leads to quantities with many figures and a power as -18 not easily comparable with the current usage.

\todo[inline]{cf. example HESS data shown in Aladin}


\paragraph{t\_gti}

The searching criteria in terms of time coverage require the list of stable/good time intervals to pick appropriate datasets.
t\_min, t\_max is the global time span but t\_gti could contain the list of GTI as a T\_MOC description following the Multi-Order-Coverage (MOC) IVOA standard \citep{2022ivoa.spec.0727F}.
This element could then be compared across data collections to make the data set selection via simple intersection or union operations in T\_MOC representation.
On the data provider's side, the T-MOC element can be computed from the Stable/Good Time Interval table in OGIP or GADF to produce the ObsCore t\_gti field.




\subsubsection{Access and Description of IRFs}

Each IRF file can have an Access object from ObsCore DM to describe a link to the IRF part of the data file.
This can be reflected in an extension of ObsTAP TAP\_SCHEMA.

In the TAP service we could add an IRF Table, with the following columns:

\begin{itemize}
    \item event-list datapublisher\_id
    \item irf\_type, category of response: EffectiveArea, PSF, etc.
    \item irf\_description, one line explanation for the role of the file
    \item Access.url, URL to point to the IRF
    \item Access.format, format of IRF
    \item Access.size, size of IRF file
\end{itemize}



\subsection{Event-list Context Data Model}
\label{sec:EventListContext}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{figures/EventListContext}
\caption{event-list Context Data Model. Notes: STIs and GTIs are slightly different concepts, and multiplicities should be adapted, energy is to specific for an event (intensity?), more products may be attached to a STI/GTI or to IRF.}
\label{fig:EventListContext}
\end{figure}

The event-list concept may include, or may be surrounded by other connected concepts. Indeed, an event-list dataset alone cannot be scientifically analysed without the knowledge of some contextual data and metadata, starting with the good/stable time intervals, and the corresponding IRFs.

The aim of the Event-list Context Data Model is to name and indicate the relations between the event-list and its contextual information. It is presented in Figure~\ref{fig:EventListContext}.


\subsection{Use of Datalink for HE products}
\label{sec:datalink}
There are two options to provide an access to a full event-bundle package.

In the first option, the "event-bundle" dataset (\ref{sec:event-bundlle-or-list}) exposed in the discovery service  contains all the relevant information, e.g. several frames in the FITS file, one corresponding to the event-list itself, and the others providing good/stable time intervals, or any IRF file. This is what was done in the current GADF data format (see \ref{sec:GADF}). In this option, the content of the event-list package should be properly defined in its description: what information is included and where is it in the dataset structure? Obviously the Event-list Context Data Model (see \ref{sec:EventListContext}) would be useful to provide that.

In the second option we provide links to the relevant information from the base "event-list" (\ref{sec:event-bundlle-or-list}) exposed in the discovery service. This could be done using Datalink and a list of links to each contextual information such as the IRFs. The Event-list Context Data Model (see \ref{sec:EventListContext}) would provide the concepts and vocabulary to characterise the IRFs and other information relevant to the analysis of an event-list. These specific concepts and terms describing the various flavors of IRFs and GTI will be given in the semantics and content\_qualifier FIELDS of the DataLink response to qualify the links. The different links can point to different
dereferencable URLs or alternbatively to different fragments of the same drefereencable URL as stated by the DataLink specification.


\todo[inline]{To be completed: show an example ?}



\bibliography{VOHE-Note, ivoatex/docrepo, ivoatex/ivoabib}
%\bibliographystyle{}

\appendix

\section{Changes from Previous Versions}

No previous versions yet.
% these would be subsections "Changes from v. WD-..."
% Use itemize environments.


% NOTE: IVOA recommendations must be cited from docrepo rather than ivoabib
% (REC entries there are for legacy documents only)

\end{document}
